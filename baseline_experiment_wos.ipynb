{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a99daf7-1dcb-4739-b78d-13bf7ab920f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1169c7-f662-4363-b868-f781ad4b405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_data', 'label', 'label_level_1', 'label_level_2'],\n",
      "        num_rows: 11967\n",
      "    })\n",
      "})\n",
      "{'input_data': 'The aim of this study was to investigate (a) the behavioral cues that are displayed by, and trait judgments formed about, anxious interviewees, and (b) why anxious interviewees receive lower interview performance ratings. The Behavioral Expression of Interview Anxiety Model was created as a conceptual framework to explore these relations. We videotaped and transcribed mock job interviews, obtained ratings of interview anxiety and interview performance, and trained raters to assess several verbal and nonverbal cues and trait judgments. The results indicated that few behavioral cues, but several traits were related to interviewee and interviewer ratings of interview anxiety. Two factors emerged from our factor analysis on the trait judgments-Assertiveness and Interpersonal Warmth. Mediation analyses were performed and indicated that Assertiveness and Interpersonal Warmth mediated the relation between interview anxiety and interview performance. Speech rate (words spoken per minute) and Assertiveness were found to mediate the relation between interviewee and interviewer ratings of interview anxiety. Overall, the results indicated that interviewees should focus less on their nervous tics and more on the broader impressions that they convey. Our findings indicate that anxious interviewees may want to focus on how assertive and interpersonally warm they appear to interviewers. To our knowledge, this is the first study to use a validated interview anxiety measure to examine behavioral cues and traits exhibited by anxious interviewees. We offer new insight into why anxious interviewees receive lower interview performance ratings.\\n', 'label': 10, 'label_level_1': 2, 'label_level_2': 2}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_down_dir = \"/root/storage/nas/JH_server/2025/Synthetic_data/0_dataset\"\n",
    "\n",
    "dataset = load_dataset(f\"{data_down_dir}/web_of_science\")\n",
    "# 예시 출력\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909b3ac7-5446-49ae-8fee-d255a2fcdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_counter = defaultdict(int)\n",
    "\n",
    "for row in dataset['train']:\n",
    "    label_counter[row['label']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac087ede-097f-4734-92ef-113d21083987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086e228a300740968d20cb0d6c093edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9573 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab92129a6ed45a0aa6ca46a495ff8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_334237/810534724.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1379' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1379/1500 03:36 < 00:19, 6.35 it/s, Epoch 9.19/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.362813</td>\n",
       "      <td>0.729741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.898646</td>\n",
       "      <td>0.776107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.779320</td>\n",
       "      <td>0.800334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.801170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.047900</td>\n",
       "      <td>0.787704</td>\n",
       "      <td>0.799081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.047900</td>\n",
       "      <td>0.784873</td>\n",
       "      <td>0.794904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.047900</td>\n",
       "      <td>0.824523</td>\n",
       "      <td>0.790309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.047900</td>\n",
       "      <td>0.855407</td>\n",
       "      <td>0.799499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.047900</td>\n",
       "      <td>0.856276</td>\n",
       "      <td>0.799081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.870539</td>\n",
       "      <td>0.802423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.911225</td>\n",
       "      <td>0.806182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.915721</td>\n",
       "      <td>0.805764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.923448</td>\n",
       "      <td>0.809942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='/root/storage/nas/JH_server/cache/')\n",
    "\n",
    "def preprocess(inputs):\n",
    "    return tokenizer(inputs['input_data'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_labels =  len(label_counter)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, cache_dir='/root/storage/nas/JH_server/cache/')\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)\n",
    "\n",
    "    return acc\n",
    "\n",
    "args = TrainingArguments(\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1500dfd-20e1-4351-8cfa-aab127558b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
